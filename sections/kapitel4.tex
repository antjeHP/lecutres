\section{Sortieren}
\subsection{Das Sortierproblem}
\paragraph{Gegeben} $n \in \N$ verschiedene Zahlen $z_1,\ldots,z_n \in \R$.
\paragraph{Gesucht} Permutation $\pi_1,\ldots,\pi_n$, so dass $z_{\pi_1} < \ldots < z_{\pi_n}$
\begin{definition}[Permutation]
Eine Permutation $\pi$ von $\{1,2,\ldots, n\}$ ist eine bijektive Abbildung von $\{1,2,\ldots,n\} $ auf sich selbst. Wir schreiben $\pi(k)=\pi_k$ für $k=1,\ldots,n$
\end{definition}
\begin{remark}
Da wir die Zahlen $z_1,\ldots,z_n$als verschieden annehmen ist das Sortierproblem eindeutig lösbar.
\end{remark}
\begin{algorithm}
\label{alg:bruteforce}
\caption{Brute-Force}
Probiere so lange alle möglichen Permutationen durch, bis die gewünschte Sortierung vorliegt
\end{algorithm}
\begin{theorem}
Es gibt $n! = n(n-1)\ldots 2 \cdot 1$ Permutationen der Menge $\{1,2,\ldots,n\} $
\end{theorem}
\begin{proof}
Wir haben 
\begin{itemize}
	\item $n$ Möglichkeiten die erste Zahl auszuwählen
	\item $n-1$ Möglichkeiten die zweite Zahl auszuwählen
	\item \ldots
	\item 1 Möglichkeit die letzte Zahl auszuwählen
\end{itemize}
woraus die Behauptung folgt.
\end{proof}
Im schlimmsten Fall (worst case) muss der Algorithmus \ref{alg:bruteforce} also
\[
	(n-1)\cdot n!
\]
Vergleiche durchführen. Da dies extrem aufwändig sein kann, betrachten wir im Folgenden einem Algorithmus, der die transitive Struktur
\[
x<y \text{ und } y<z \implies x<z
\]
der Ordnungsrelation ausnutzt. \\
\begin{algorithm}[H]
\caption{Bubblesort}
\label{alg:bubblesort}
\KwData{Menge $S_n=\{z_1,\ldots,z_n\}$}
\KwResult{Sortierte Menge $S^{\pi}=\{z_{\pi_1},\ldots,z_{\pi_n}\} $}
\For{$k \gets n$ \KwTo $1$}{
$z_{\pi_k}=\max( S_k)$\\
$S_{k-1}=S_k \setminus \{z_{\pi_k}\}$
}
\end{algorithm}
\begin{example}
Die zu sortierende Menge ist: $\{4,1,2\}$.
\begin{itemize}
	\item $S_3=\{4,1,2\}$, $k=3$, $z_{\pi_3}=4$
	\item $S_2=\{1,2\}$, $k=2$, $z_{\pi_2}=2$
	\item $S_1=\{1\}$, $k=1$, $z_{\pi_1}=1$
\end{itemize}
Die sortierte Liste ist: $\{1,2,4\}$.
\end{example}
Um den Aufwand von Algorithmen zu untersuchen, beschränken wir uns auf das asymptotische Verhalten für große n.
\begin{definition}[Landau-Notation]
Wir schreiben
\begin{itemize}
	\item $f(x)= \mathcal{O}(g(x))$, falls Zahlen $C>0, x_0>0$ existieren, so dass: $|f(x)|\le Cg(x), \forall_{x>x_0}$.
	\item $f(x)=\Omega(g(x))$, falls Zahlen $C>0, x_0>0$ existieren, so dass $|f(x)|\ge Cg(x) \forall_{x>x_0}$.
	\item $f(x)=o(g(x))$, falls für jedes $c>0$ ein $x_0>0$ existiert, so dass 
		$|f(x)|\le cg(x), \forall_{x>x_0}$.
	\item $f(x)= \Theta(g(x))$, falls $f(x)=\mathcal{O}(g(x))$, $g(x)= \mathcal{O}(f(x))$
\end{itemize}
\end{definition}
\begin{remark}
Genau dann wenn Beziehung der Landau-Notation
\begin{itemize}
	\item $f(x)=\mathcal{O}(g(x)) \iff \limsup_{x \to \infty} |\frac{f(x)}{g(x)}|\le C<\infty$
	\item $f(x)= \Omega(g(x)) \iff \liminf_{x \to \infty} |\frac{f(x)}{g(x)}|>0$
	\item $f(x)= o(g(x)) \iff \lim_{x \to \infty} |\frac{f(x)}{g(x)}| =0$
	\item $f(x)= \Theta(g(x)) \iff f(x)= \mathcal{O}(g(x)), f(x)= \Omega(g(x))$
\end{itemize}
\end{remark}
\begin{example}
Es gilt $\sin(x) = \mathcal{O}(1)$, da $|\sin(x)|\le 1$ für alle $x \in  \R$. Bei Polynomen gilt die Laufzeit ist die höchste Potenz, sofern $ x\ge 1$.
\end{example}
\begin{definition}
Der Aufwand eines Algorithmus ist die kleinste obere Schranke für das betrachtete Aufwandsmaß
\end{definition}
Für uns ist der Speicherbedarf irrelevant und benutzen als Aufwandsmaß für den Rechen die Anzahl der benötigten Vergleiche. \\
In der k-ten Iteration des Bubblesort-Algortihmus \ref{alg:bubblesort} müssen k Vergleiche ausgeführt werden. Das heißt, der Gesamtaufwand des Algorithmus ist:
\[
\sum_{k=1}^{n-1}k=\frac{n(n-1)}{2}= \mathcal{O}(n^2)
\]
Dies ist eine drastische Verbesserung im Vergleich zu $\mathcal{O}(n(n!))$ von Algorithmus \ref{alg:bruteforce}
\subsection{Mergesort}
Zu erst wollen wir folgendes beobachten:
\begin{lemma}
	Gegeben seien zwei sortiere Mengen
	\begin{itemize}
		\item $S_x=\{x_1<\ldots<x_m\}$
		\item $S_y=\{y_1<-\ldots<y_n\}$
	\end{itemize}
Dann lässt sich die Menge $S=S_x \cup S_y$ mit linearem Aufwand sortieren. Genauer werden $m+n-1$ Vergleiche benötigt.			
\end{lemma}
\begin{proof}
Konstruktiv, durch den entsprechenden Algorithmus.
\end{proof}
\begin{algorithm}[H]
\label{alg:merge}
\caption{Merge}
\KwData{Sortierte Mengen $S_x$ und $S_y$}
\KwResult{Sortierte Menge $S= S_x \cup S_y$}
Initialisiere $i=j=k=1$ \\
\While{$i\le m$ und $j\le n$}{
\If{$x_i < y_j$}{

$z_k=x_{i}$ \\
$i=i+1$
}
\Else{
$z_k=y_j$ \\
$j=j+1$

}
$k=k+1$
}
\For{$l \gets 0$ \KwTo $m-i$}{
$z_{k+l}=x_{k+l}$
}
\For{$l \gets 0$ \KwTo $n-j$}{
$z_{k+l}=y_{k+l}$
}
\end{algorithm}
Basierend auf dieser Beobachtung können wir eine divide-and-conquer Strategie angeben um Mengen der Länge $n=2^{m}, m \in  \N$ zu sortieren. \\
\begin{algorithm}[H]
	\label{alg:mergesort}
	\caption{Mergesort}
	\KwData{Menge $S= \{z_1,\ldots,z_n\}$}
	\KwResult{Sortierte Menge $S^{\pi}= \{z_{\pi_1} < \ldots < z_{\pi_n}\} $}
	\If{$n=1$}{
	$S^{\pi}=S$
	}
	\Else{
		\begin{itemize}
			\item Sortiere \\
	$L=\{z_1,\ldots, z_{\frac{n}{2}}\}$ \\
	$R= \{z_{\frac{n}{2}+1},\ldots, z_n\}$
	mittels Mergesort zu $L^{\pi}$ und $R^{\pi}$ \\
	\item Sortiere $L^{\pi} \cup R^{\pi}$ mittels Merge-Algorithmus \ref{alg:merge} zu $S^{\pi}$
	\end{itemize}
	}
\end{algorithm}
\begin{example}
Mergesort der Menge $\{20,7,84,31,71,42,18,10\} $
\end{example}
\begin{remark}
Da Mergesort sich selbst aufruft, sprechen wir von einem \emph{rekursiven Algorithmus}. Im Allgemeinem ist es schwierig zu beurteilen, ob solche Algorithmen terminieren. Im Fall von Mergesort ist der Fall jedoch klar, da die Rekursion im Fall $n=1$ abgebrochen wird.
\end{remark}
\begin{theorem}
	\label{thm:mergesort}
	Der Aufwand von Mergesort ist $\mathcal{O}(n \log n)$
\end{theorem}
\begin{proof}
Bezeichne A(n) den Aufwand für das Sortieren einer $n=2^{m}$ elementigen Menge mittels Mergesort. Dann gilt:
\begin{align*}
	A(1)&=0 \\
	A(n)&=n-1 + 2A(\frac{n}{2})
\end{align*}
Auflösen der Rekursion ergibt:
\begin{align*}
	A(n)
	&=n-1+2A(\frac{n}{2})\\
	&=2n-1-2+4A(\frac{n}{4})\\
	&=\ldots \\
	&=mn-\sum_{i=0}^{m-1}2^{i} \\
	&= mn- \frac{1-2^{m}}{1-2} \\
	&= (m-1)n+1
\end{align*}
$m=\log_2(n)=\frac{\log(n)}{\log(2)}$ impliziert die Behauptung
\end{proof}
$\log n \ll n$, also ist die Verbesserung von $\mathcal{O}(n^2)$ nach $\mathcal{O}(n\log(n)$ signifikant.
Man nennt das Wachstum auch beinahe linear.

\begin{remark}
Die Implementierung von Mergesort als in-place-Algorithmus ist je nach Datenstrukturen trickreich. Deshalb wird oft nicht in-place implementiert, weshalb für jeden "divide"-Schritt zusätzlicher Speicherplatz implementiert werden muss.
\end{remark}
\subsection{Quicksort}
\paragraph{Idee:} Divide-and-Conquer Strategie basierend auf dem Inhalt der zu sortierenden Liste.
\begin{algorithm}
\label{alg:quicksort}
\caption{Quicksort}
\KwData{Menge $S=\{z_1,\ldots, z_n\}$}
\KwResult{Sortierte Menge $S^{\pi}=\{z_{\pi_1},\ldots,z_{\pi_n}\}$}
	Wähle ein Pivot-Element $x \in S$ \\
	Bestimme eine Permutation $\pi$, so dass $x=z_{m_{\pi}}$\\
	\If{$L=\{z_{\pi_1},\ldots,z_{\pi_{m-1}} \neq \emptyset$\}}
	{
	Sortiere $L$ zu $L^{\pi}$ mittels Quicksort
	}

	\If{ $R=\{z_{\pi_{m+1}},\ldots,z_{\pi_n}\} \neq \emptyset$}{
		Sortiere $R$ zu $R^{\pi}$ mittels Quicksort}
	Vereinige $S^{\pi}= R^{\pi} \cup \{x\} \cup L^{\pi}$
\end{algorithm}
\begin{example}
Beispiel anhand der gleichen Menge von Mergesort.
\end{example}
\begin{lemma}
Im schlimmsten Fall ist der Aufwand von Quicksort $\mathcal{O}(n^2)$
\end{lemma}
\begin{proof}
$A(n)$ aus \ref{thm:mergesort} wird umso größer, je unterschiedlicher die Größe der beiden Teilprobleme $A(m-1)$ und $A(n-m)$ ist.
$A(n)$ ist also maximal für $m=1$ oder $m=n$, also wenn das Pivot-Element das größte oder kleinste Element ist. Dann gilt:
\[
A(n)=n-1+A(n-1)
\]
Der Rest erfolgt Analog zu der Aufwandserklärung von Bubblesort \ref{alg:bubblesort}:
\[
A(n)=\frac{n(n-1)}{2}= \mathcal{O}(n^2)
\]
Damit ist der Aufwand gezeigt.
\end{proof}
\begin{theorem}
	Alle Permutationen der Zahlen $\{1,2,\ldots,n\}$ seien gleich wahrscheinlich. Dann benötigt Quicksort im Durchschnitt $\mathcal{O}(n\log(n)$ Vergleiche zum sortieren von Zahlen.
\end{theorem}
\begin{proof}
Sie $\Pi$ die Menge aller Permutationen von $\{1,2,\ldots,n\}$ und sei $A(\pi), \pi \in \Pi$ die Anzahl Vergleiche um eine Permutation $\pi$ mittels Quicksort zu sortieren. Der durchschnittliche Aufwand ist:
\[
\overline{A}(n)=\frac{1}{n!} \sum_{\pi \in \Pi}A(\pi)
\]
O.B.d.A: Sei das erste Element das Pivotelement. Definiere:
\[
\Pi_k = \{\pi \in  \Pi | \pi_1=k \}, k=1,\ldots,n 
\]
mit
\[
|\Pi_k| =(n-1)!, k=1,\ldots,n
\]
Für $k$ fix und $\pi \in \Pi_k$ teilt Quicksort im ersten Aufruf in zwei Mengen
\begin{align*}
	\pi_< &= \{\text{Permutationen von } 1,2,3\ldots,k-1\} \\
	\pi_> &= \{\text{Permutationen von } k+1,\ldots,n\}  
\end{align*}
Analog zu \ref{thm:mergesort} folgt
\[
A(\pi) = n-1+ A(\pi_<) + A(\pi_>)
\]
und
\begin{equation}
	\label{eqn:quicksort1}
\sum_{\pi \in  \pi_k} A(\pi) = (n-1)! (n-1) + \sum_{\pi \in  \Pi_k} A(\pi_<)+ \sum_{\pi \in \Pi_k}A(\pi_>)
\end{equation}
Wenn $\pi$ alle Permutationen aus $\Pi_k$ durchläuft, entstehen für $\Pi_<$ alle Permutationen von $\{1,2,\ldots,k-1\}$. 
Dabei tritt jede Permutation genau $\frac{(n-1)!}{(k-1)!}= \frac{|\Pi_k|}{\Pi_<}$ mal auf.
\begin{equation}
\label{eqn:quicksort2}
\implies \sum_{\pi \in \Pi_k} A(\pi_<) = \frac{(n-1)!}{(k-1)!} \sum_{\pi \in \Pi_k}A(\pi_<) = 
(n-1)!  \overline{A}(k-1)
\end{equation}
Analog:
\begin{equation}
\label{eqn:quicksort3}
\sum_{\pi \in \Pi_k}A(\pi_>)= (n-1)! \overline{A}(n-k)
\end{equation}
Zusammensetzen: 
\begin{align*}
\overline{A}(n)
&= \frac{1}{n!}\sum_{\pi \in  \Pi}A(\pi) \\
&= \frac{1}{n!}\sum_{k=1}^{n}\sum_{\pi \in \Pi_k}
\end{align*}
Unter Verwendung der Gleichungen \ref{eqn:quicksort1}, \ref{eqn:quicksort2}, \ref{eqn:quicksort3} folgt weiter:
\begin{align*}
&=\frac{1}{n!}\sum_{k=1}^{n}(n-1)! (n-1 + \overline{A}(k-1) +\overline{A}(n-k) \\
&= n-1+\frac{1}{n}\sum_{k=1}^{n}\overline{A}(k-1) +\overline{A}(n-k) \\
&= n-1 + \frac{2}{n}\sum_{k=0}^{n-1}\overline{A}(k)
\end{align*}
mit Startwerten $\overline{A}(0)=\overline{A}(1)=0$. \\
Per Induktion folgt:
\begin{align*}
\overline{A}(n) 
&= 2(n+1) \sum_{i=1}^{n}\frac{1}{i}-4n \\
&\le 2(n+1) (1+ \int_{1}^{n} \frac{1}{x}dx -4n \\
&= 2(n+1) (1+\log(n)) -4n \\
&= 2(n+1) \log(n) -2(n-1) \\
&= \mathcal{O}(n\log(n))
\end{align*}
\end{proof}
\subsection{Untere Schranke für das Sortierproblem}
\paragraph{Frage:} Gibt es einen Sortieralgorithmus, der schneller ist als $\mathcal{O}(n\log(n))$?

\begin{theorem}
	\label{thm:sortierverfahren}
Jedes deterministische Sortierverfahren, das auf paarweisen Vergleichen basiert und keine Vorkenntnisse über die zu sortierende Menge hat, benötigt im schlimmsten Fall mindestens $\log_2(n!)$ Vergleiche zum Sortieren von $n$ verschiedenen Zahlen.
\end{theorem}
Bevor wir den Beweis machen gibt es noch einige Hinweise, die wir für den Beweis benötigen.
\begin{remark}
	Es gilt:
	\begin{itemize}
		\item  $\log_2(n!) = \Theta(n\log(n)$
		\item $\log_2(n) \le \log_2(n^{n}) = n\log_2(n) = \frac{1}{\log(2)}n \log n$
		\item $\log(n!) \ge \log(\frac{n}{2}\ldots \frac{n}{2}) \ge \log((\frac{n}{2})^{\frac{n}{2}}= \frac{n}{2}\log(\frac{n}{2}) = \frac{n}{2}\log(n) - n\log(2)$
	\end{itemize}
\end{remark}
Zum Beweis benötigen wir einen Entscheidungsbaum. Dieser kann jedem Sortieralgorithmus mit paarweisen Vergleichen zugeordnet werden, und illustriert das Verhalten des Algorithmus:
\begin{itemize}
	\item Innere Knoten des Entscheidungsbaums sind Vergleiche im Algorithmus.
	\item Ein Weg von der Wurzel zu einem Blatt entspricht der zeitlichen Abfolge der Vergleiche. Ein Weitergeben nach rechts illustriert einen richtigen Vergleich, nach links einen falschen.
	\item Die $n!$ Blätter des Baumes stellen die Permutationen der zu sortierenden Menge dar, die jeweils zur Abfolge der Vergleiche gehört.
\end{itemize}
\begin{example}
Baum für $\{z_1,z_2,z_3\}$: \\
%\begin{center}
%\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1,
% level distance = 1.5cm}]
%\node [arn_n] {$z_1<z_2$}
%child{ node [arn_r] {$z_1<z_3$}
%         child{ node [arn_n] {$z_2<z_3$}
%          	child{ node [arn_r] {$\{z_3,z_2,z_1\} $} edge from parent node[above left]
%                        {nein}} %for a named pointer
%							child{ node [arn_x] {$\{z_2,z_3,z_1\} $}}
%       }
%        child{ node [arn_n] {$\{z_2,z_1,z_3\} $}}
% }
%  child{ node [arn_r] {47}
%           child{ node [arn_n] {38}
%							child{ node [arn_r] {36}}
%							child{ node [arn_r] {39}}
%            }
%            child{ node [arn_n] {51}
%							child{ node [arn_r] {49}}
%							child{ node [arn_x] {}}
%            }
%		}
%;
%\end{tikzpicture}
%\end{center}
\end{example}
\begin{proof}
\ref{thm:sortierverfahren} \\
\underline{Idee:} Finde eine untere Schranke für die Tiefe des Baumes. Dies ist die Mindestanzahl der im schlimmsten Fall mötigen Vergleiche.
Im besten Fall ist ein Baum ausbalanciert, d.h. alle Blätter haben die gleiche Tiefe $m$ und wir haben $2^{m}$ Blätter. Da wir $n!$ Permutationen haben (also Blätter), muss $2^{m}=n!$ gelten. Es folgt $m \log_2(n!)$
\end{proof}
Satz \ref{thm:sortierverfahren} sagt, dass Mergesort in der asymptotischen Laufzeit optimal ist. 
\paragraph{Frage:} Können wir etwas Ähnliches für Quicksort zeigen?

\begin{theorem}Alle Permutationen der Zahlen $z_1,\ldots,z_n$ seien gleich wahrscheinlich. Dann benötigen Sortierverfahren wie in Satz \ref{thm:sortierverfahren} im Mittel $\log_2(n!)$ Vergleiche.
\end{theorem}
\begin{proof}
Die mittlere Höhe $\overline{H}(z)$ des Entscheidungsbaums $\tau$ ist gegeben als der Mittelwert der Tiefen $t_{\tau}$ aller seiner Blätter $v$.
Sei $B(\tau)$ die Menge der Blätter und $\beta(\tau) = |B(\tau)|$ Dann gilt:
\[
\overline{H}(\overline{c})=\frac{1}{\beta(\overline{c})}\sum_{y=B(\overline{c}} t\overline{c}(v)
\]
Zu zeigen ist, dass:
\[
\overline{H}(\overline{c}) \ge \log_2(\beta(\overline{c}))
\]
Vollständige Induktion über die Höhe des Entscheidungsbaums $\overline{c}$.
\begin{itemize}[label=$\lozenge$, itemsep=2ex]
	\item Induktionsverankerung:$H(\overline{c})=0$ \\
		$\overline{c}$ besteht nur aus seiner Wurzel. $\implies \overline{H}(c)= \frac{1}{1}\sum_{y \in B(\tau)} t_{\tau}(v)=0= \log_2(1)$
	\item Induktionsschritt: $H(\overline{c})\le h-1 \implies H(\overline{c})=h$ \\
		Seien $\tau_l , \tau_r$ die Wurzeln des linken bzw. rechten Teilbaums zum Wurzelknoten von c:
\begin{center}

	%Graph missing

\end{center}
\paragraph{Beobachte:}
\begin{itemize}
	\item $H(\tau_l), H(\tau_r) <h$
	\item $B(\tau)= B(\tau_l) \cup B(\tau_r)$
	\item $B(\tau_l) \cap B(\tau_r) = \emptyset$
	\item $\beta(\overline{c})= \beta(\tau_l) + \beta(\tau_r)$
	\item $\beta(\tau_l), \beta(\tau_r) \ge 1$
	\item $t_{\tau}(v)= t_{\tau_l}(v)+1$
	\item $t_{\tau}(v)= t_{\tau_r}(v)+1$
\end{itemize}
Rechne:
\begin{align*}
\overline{H}(\tau) 
&= \frac{1}{\beta(\tau)}\sum_{v \in B(\tau)} t_{\tau}(v) \\
&= \ldots \\
&\ge 1+\frac{1}{b} \min_{x \in [0,b]} (x \log_2(x) + (b-x) \log_2(b-x))
\end{align*}
Da
\begin{align*}
f'(x)
&= \log_2(x) -\log_2(b-x) \\
&= \log_2(\frac{x}{b-x}) &=0
\end{align*}
genau dann, wenn $x=\frac{b}{2}$, ist dies die einzige Möglichkeit, wo das Minimum in $(0,b)$ angenommen werden kann. Die anderen Möglichkeiten sind die Intervallrandpunkte, was ausgeschlossen ist.
Also gilt:
\begin{align*}
	\overline{H}(\tau) 
	&\ge 1+ \log_2(\frac{b}{2}) \\
	&= 1+ \log_2(\beta(\tau)) -1 \\
	&= \log_2(\beta(\tau))
\end{align*}
\end{itemize}
\end{proof}

