\section{Fehleranalyse}
\subsection{Rechnerarithmetik}
\begin{example}
Betrachte $F=F(10,5,-4,5)$ und Maschinenzahlen
\begin{align*}
x=2.5684 \cdot 10^{0}= 2.56840000 \\
y=3.2791 \cdot 10^{-3} = 0.0032791
\end{align*}
Es gilt:\\
\begin{center}
$\begin{rcases}
	x+y= 2.5716792 \\
	x-y= 2.5651209 \\
	x \cdot y= 0.00842204044 \\
	\frac{x}{y}= 783.2637004
\end{rcases} \not\in F$
\end{center}
\end{example}
\begin{remark}
Die Menge $F(b,t,e_{min},e_{max})$ ist nicht abgeschlossen bezüglich der Grundrechenarten und können somit im Allgemeinen nicht im Computer implementiert werden.
\end{remark}
\paragraph{Lösung}
Wir runden das Ergebnis und implementieren so eine Pseudoarithmetik.
Das bedeutet, wir ersetzen $\circ \in  \{+,-,\cdot, \div\}$ durch $\boxcircle \in \{\boxplus, \boxminus, \boxdot, \boxslash\}$ definiert durch:
\begin{equation}\label{eqn:rundung}
x \boxcircle y \coloneqq rd (x \circ y)
\end{equation}
Auf Hardwareebene wird üblicherweise mit einer längeren Matisse gearbeitet und dann normalisiert und gerundet. Dies entspricht dem IEEE 754 Standard.
\begin{remark}
Für $|x|,|y|, |x \circ y| \in [z_{min},z_{max}]$ impliziert \eqref{eqn:rundung}, dass
\[
	\frac{|x \boxcircle y - x \circ y|}{|x \circ y|}= \frac{rd(x \circ y - x \circ y)}{|x \circ y|} \le \varepsilon_{mach}
\]

Das bedeutet, dass $\boxcircle$ im Computer bestmöglich umgesetzt ist.
\end{remark}
\begin{example}
Betrachte $F=F(10,5,-4,5)$ \\
\begin{itemize}
	\item Setze:
\begin{itemize}
	\item $a=0.98765$
	\item $b=0.012424$
	\item $c=-0.0065432$
\end{itemize}
Dann gilt:
\[
	(a+b)+c=a+(b+c)=0.9925208
\]
Numerisch gilt:
\[
(0.98765 \boxplus 0.012424)\boxminus 0.0065432= rd(0.9935568)=0.99356
\]
und
\[
0.98765 \boxplus (0.012424\boxminus 0.0065432)=rd(0.9935308)=0.99353
\]
\item Setze
	\begin{itemize}
		\item $a=4.2832$
		\item $b=-4.2821$
		\item $5.7632$
	\end{itemize}
Dann gilt
\[
(a+b)\cdot c = a \cdot c + b \cdot c = 0.006339520000001
\]
Numerisch gilt:
\end{itemize}
\end{example}
\begin{remark}
Mathematisch äquivalente Algorithmen auf Fließkommazahlen können je nach Implementierung zu wesentlich unterschiedlichen Ergebnissen führen, selbst wenn die Eingangszahlen exakt dargestellt werden.
\end{remark}
\subsection{Auslöschung}
Unglücklicherweise pflanzen sich numerische Fehler, zum Beispiel durch Rundung im Verlauf eines Algorithmus fort.

\begin{lemma}[Fehlerfortpflanzung]
Es seien $x,y \in \R$ mit Datenfehlern $\Delta x, \Delta y \in  \R$ behaftet, welche\[
|\frac{\Delta x}{x}| , |\frac{\Delta y}{y}| \ll 1
\]
erfüllen. Für $\circ \in  \{+,-,\cdot, \div\}$ gilt für den fortgepflanzten Fehler
\[
\Delta (x \circ y) \coloneqq (x+ \Delta x) \circ (y + \Delta y) - x \circ y \text{, }
\]
dass
\begin{equation*}
\begin{split}
\frac{\Delta (x \pm y)}{x \pm y} = \frac{x}{x \pm y}\frac{\Delta x}{x} \pm \frac{y}{x \pm y} \frac{\Delta y}{y} \\
\frac{\Delta (x \cdot y)}{xy} \approx \frac{\Delta x}{x}+ \frac{\Delta y}{y} \\
\frac{\Delta (\frac{x}{y})}{\frac{x}{y}} \approx \frac{\Delta x}{x}-\frac{\Delta y}{y}
\end{split}
\end{equation*}
Hierbei bedeutet "$\approx$", dass Terme mit $(\Delta x)^2, (\Delta y)^2, \Delta x \Delta y$ vernachlässigt werden.
\end{lemma}
\begin{proof}
Übung.
\end{proof}
Für $\cdot,\div$ addieren bzw. subtrahieren sich die Fehler.
\paragraph{Achtung:} Ist $|x \pm y|$ wesentlich kleiner als $|x|$ oder $|y|$, kann der relative Fehler massiv verstärkt werden. 
Dieses Phänomen heißt Auslöschung.\\
Bei der Konstruktion von Algorithmen sollte Auslöschung möglichst vermieden werden.
\begin{example}\label{eg:pq}
Betrachte \\
\begin{equation}
\label{eqn:pq}
x^2-2px+q=0
\end{equation}
mit Lösungen:
\[
x_{1,2} = p \pm \sqrt[2]{p^2-q} 
\]


\begin{algorithm}[H]
 \label{alg:nst1}
 \caption{naive Nullstellenberechung}
 \KwData{$p,q \in \R$ so ,dass \eqref{eqn:pq} lösbar ist}
 \KwResult{Nullstellen $x_1,x_2$ von \eqref{eqn:pq}.}
 $d=\sqrt{p\cdot p -q}$ \\
 $x_1= p+d$ \\
 $x_2= p-d$
 \end{algorithm}
 Mit $p=100$, $q=1$ in dreistelliger, dezimaler Gleitkommaarithmetik ergibt sich:
 \begin{equation*}
	 \begin{split}
		 d= \sqrt{10000-1}= \sqrt{rd(9999)}=\sqrt{10000} = 100 \\
		 x_1=100+100=200 \\
		 x_2= 100-100 =0
	 \end{split}
 \end{equation*}
Die exakten Werte sind  $x_1 \approx 199.99, x_2 \approx 0.00500$, welche in dreistelliger, dezimaler Gleitkommaarithmetik als $x_1=200,x_2=0$ dargestellt werden. \\
Gemäß \ref{thm:emach} ist die Maschinengenauigkeit:
\[
\varepsilon_{mach} = \frac{1}{2}b^{1-t}= \frac{1}{2}10^{1-3}=0.005
\]
Der relative Fehler $\frac{|0-0.005|}{|0.005|}=1$ ist also zu 100 Prozent falsch.
\end{example}
Wir können die Genauigkeit von $x_2$ erhöhen, indem wir den Wurzelsatz von Vieta $x_1x_2=q$ verwenden.\\
\begin{algorithm}[H]
 \caption{verbesserte Nullstellenberechnung}
 \KwData{$p,q \in  \R$ so ,dass \eqref{eqn:pq} lösbar ist.}
 \KwResult{Nullstellen $x_1,x_2$ \eqref{eqn:pq}.}
 $d= \sqrt[2]{p \cdot p -q}$ \\
 \If{$q \ge 0$}{$x_1=p+d$
 \Else{$x_1=p-d$}}
 $x_2=\frac{q}{x_1}$
 \end{algorithm}
 Wir erhalten nun $d=100, x_1=200,x_2=\frac{1}{200}=0.005$

\subsection{Vorwärts- und Rückwertsanalyse}
Abstrakt gesehen entspricht das Lösen eines Problems dem Auswerten einer Funktion $f$.
Beim numerischen Auswerten von $f$ können verschiedene Fehler passieren:
%Hier muss noch das Diagramm von Vorlesung 5 hin.

\begin{definition}[Fehlerarten]
Sei $D \subset \R$ eine Menge von Eingangsdaten und $W \subset \R$ die Menge der möglichen Ergebnisse. Wir unterscheiden folgende Fehlerarten bei der Auswertung von $f \colon D \to W$.
\begin{itemize}
\item \underline{Datenfehler} Typischerweise sind die Eingangsdaten $x \in D$ nicht exakt,
sondern mit einem Datenfehler $\Delta x$ behaftet. Die gestörten Eingangsdaten $\tilde{x}=x+ \Delta x$ produzieren einen Fehler $f(x+\Delta x)-f(x)$.
\item \underline{Verfahrensfehler} Exakte Verfahren enden bei exakter Rechnung nach endlich vielen Operationen mit dem exakten Ergebnis.
Näherungsverfahren enden in Abhängigkeit bestimmter Kriterien mit einer Näherung $\tilde{y}$ für die Lösung $y \in W$.
\item \underline{Rundungsfehler} Verursacht durch Maschinenzahlen und Rundungen während der Arithmetik.
\end{itemize}
\end{definition}
Ein Teilgebiet der Numerik versucht diese Fehler durch eine Fehleranalyse zu quantifizieren.
Hierzu verfolgt man die Auswirkungen von allen Fehlern, die in den einzelnen Schritten vorkommen können.
\begin{definition}[Analysearten]
Bei der \emph{Vorwärtsanalyse} wird der Fehler von Schritt zu Schritt verfolgt und der akkumulierte Fehler für jedes Teil-Ergebnis abgeschätzt. \\
Bei der \emph{Rückwertsanalyse} geschieht die Verfolgung des Fehlers hingegen so, dass jedes Zwischenergebnis als exakt berechnetes Ergebnis zu gestörten Daten interpretiert wird, d.h , der akkumulierte Fehler wird als Datenfehler interpretiert.
\end{definition}
\begin{example}
Betrachte $f(x,y)=x+y$ \\
\begin{itemize}
	\item Vorwärtsanalyse: $\boxed{f} = x \boxplus y= (x+y)(1+\varepsilon)$
	\item Rückwärtsanalyse: $x \boxplus y= x(1+\varepsilon)+y(1+\varepsilon)= f(x(1+\varepsilon), y(1+\varepsilon)$
\end{itemize}
mit $|\varepsilon|\le \varepsilon_{mach}$
\end{example}
In der Praxis ist die Vorwärtsanalyse kaum durchführbar. 
Für die meisten Algorithmen ist, wenn überhaupt, nur eine Rückwärtsanalyse bekannt.
\begin{example}
\label{eg:rückwärtsanalyse}
Fortsetzung von Beispiel \ref{eg:pq} \\
Führe Rückwärtsanalyse durch zu Algorithmus \ref{alg:nst1} für die Betrags-kleinere Nullstelle $x_2$ durch:
Dann ist
\[
f(p,q)=p-\sqrt[2]{p^2-q}
\]
mit $p>0$. 
Für $|\varepsilon_i|\le \varepsilon_{mach}, i=1,2,3,4$ betrachten wir:
\begin{align*}
\left( p-\sqrt[2]{(p^2(1+\varepsilon_1)-q)(1+\varepsilon_2)} (1+\varepsilon_3) \right)(1+\varepsilon_4)
&= p(1+\varepsilon_4)-\sqrt[2]{(p^2(1+\varepsilon_1)-q)(1+\varepsilon_2)(1+\varepsilon_3)^2(1+\varepsilon_4)^2}\\
&= p^2(1+\varepsilon_1)(1+\varepsilon_3)^2(1+\varepsilon_4)^2-q(1+\varepsilon_2)(1+\varepsilon_3)^2(1+\varepsilon_4)^2 \\
&= \ldots \\
&= p(1+\varepsilon_4)-\sqrt[2]{p^2(1+\varepsilon_4)^2-q(1+\varepsilon_7)} \\
&= f(p(1+\varepsilon_4),q(1+\varepsilon_7))
\end{align*}
Die Abschätzung für $\varepsilon_7$ explodiert, falls $0<|q| \ll 1 < p$. Dies war in Beispiel \ref{eg:pq} der Fall.
\end{example}
\subsection{Kondition und Stabilität}
Gegeben sei eine stetige und differenzierbare Funktion
\[
f \colon \R \to \R, x \mapsto y = f(x) 
\]
Für fehlerhafte Daten $x+\Delta x$ mit kleinen Fehler $\Delta x$ gilt:
\[
\frac{f(x+\Delta x)-f(x)}{\Delta x} \approx f'(x)\text{.}
\]
Für den absoluten Datenfehler $\Delta y$ gilt:
\[
\Delta y= f(x+\Delta x)-f(x) \approx f'(x) \cdot \Delta x
\]
und für den relativen Fehler:
\[
\frac{\Delta y}{y}= \frac{f'(x)\cdot \Delta x}{f(x)}=\frac{f'(x)x}{f(x)}\cdot \frac{\Delta x}{x}
\]
\begin{definition}[Konditionszahlen]
\label{def:konditionszahl}
Die Zahl \[
K_{abs} = |f'(x)|\]
heißt \emph{absolute Konditionszahl} des Problems $x \mapsto f(x)$. 
Für $f(x)\cdot x \neq 0$ heißt
\[
K_{rel}=|\frac{f'(x) \cdot x}{f\left( x \right)}|\] 
die entsprechende \emph{relative Konditionszahl}.
Ein Problem heißt \underline{schlecht konditioniert},falls eine dieser beiden Konditionszahlen deutlich größer als 1 sind.
Ansonsten heißt das Problem \underline{gut konditioniert}.
\end{definition}
\begin{example}
Konditionierung der Addition und Multiplikation
\begin{itemize}
	\item Für die Addition $f(x)=x+a$ gilt
\[
K_{rel}= |\frac{f'(x)x}{f(x)}|= |\frac{x}{x+1}|
\]
$\implies K_{rel}$ ist groß, falls $|x+a| \ll |x|$.
\item Für die Multiplikation $f(x)=x \cdot a$ gilt
\[
K_{rel} = \frac{|f'(x)x|}{|f(x)|}= \frac{|ax|}{|ax|}=1
\]
$\implies$ Die absolute Kondition ist schlecht, falls $1 \ll q$. Die relative Kondition ist immer gut.
\end{itemize}
\end{example}
\begin{definition}
Erfüllt die Implementierung eines Algorithmus $\boxed{f}$ zur Lösung eines Problems $x \mapsto f(x)$ die Abschätzung
\[
|\frac{\boxed{f}-f(x)}{f(x)} \le C_V K_{rel} \varepsilon_{mach}
\]
mit einem mäßig großen $C_V >0$, so wird der Algorithmus \emph{vorwärtsstabil} genannt.
Ergibt die Rückwärtsanalyse $\boxed{f}(x)=f(x+\Delta x)$ mit
\[
|\frac{\Delta x}{x} \le C_R \varepsilon_{mach}
\]
mit $C_R>0$ nicht zu groß, so heißt der Algorithmus \boxed{f} \emph{rückwärtsstabil}

%Fancy Grafik Vorlesung 5 Seite 4
\end{definition}
\begin{theorem}
Jeder rückwärtsstabile Algorithmus ist auch vorwärtsstabil
\end{theorem}
\begin{proof}
Übung.
\end{proof}
Oft ist Rückwärtsstabilität einfacher nachzuweisen.
\paragraph{Faustregel}zu konditionierten Probleme
\begin{itemize}
	\item Gut konditioniertes Problem + stabiler Algorithmus \\ $\implies$ Gute numerische Resultate.
\item Schlecht konditioniertes Problem oder instabiler Algorithmus \\ $\implies$ Fragwürdige Ergebnisse.
\end{itemize}

\begin{example}
Fortsetzung von Beispiel \ref{eg:rückwärtsanalyse} \\
Die Rückwärtsanalyse hat gezeigt: Falls $0<|q| \ll 1 <p$ wird der numerische Fehler untragbar.
Unsere Abbildung ist:
\[
f(q)=p-\sqrt[2]{p^2-1}
\]
Als Konditionszahlen ergeben sich:
\begin{equation*}
K_{abs} = |f'(q)|= |\frac{1}{2\sqrt[2]{p^2-q}}| < 1
\end{equation*}
\begin{align*}
	K_{rel} 
	&=|\frac{f'(q)q}{f(q)}| \\
	&=|\frac{q}{2\sqrt[2]{p^2-q}(p-\sqrt[2]{p^2-q})(p+\sqrt[2]{p^2-q})}| \\
	&=\frac{1}{2}|\frac{p+\sqrt[2]{p^2-q}}{\sqrt[2]{p^2-q}} \\
	&\approx \frac{1}{2}|\frac{p+p}{p}| \approx 1
\end{align*}%
$\implies$ Nullstellenberechung ist ein gut konditioniertes Problem, aber  Algorithmus \ref{alg:nst1} muss instabil sein
\end{example}

